{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed3cc989",
   "metadata": {},
   "source": [
    "## IMPLEMENTING A GPT MODEL FROM SCRATCH TO GENERATE TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d40c46f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,    # Vocabulary size\n",
    "    \"context_length\": 1024, # Context length\n",
    "    \"emb_dim\": 768,         # Embedding dimension\n",
    "    \"n_heads\": 12,          # Number of attention heads\n",
    "    \"n_layers\": 12,         # Number of layers\n",
    "    \"drop_rate\": 0.1,       # Dropout rate\n",
    "    \"qkv_bias\": False       # Query-Key-Value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297708c8",
   "metadata": {},
   "source": [
    "**vocab_size** refers to a vocabulary of 50,257 words, as used by the BPE tokenizer (see chapter 2).\n",
    "\n",
    "**context_length** denotes the maximum number of input tokens the model can handle via the positional embeddings (see chapter 2).\n",
    "\n",
    "**emb_dim** represents the embedding size, transforming each token into a 768 dimensional vector.\n",
    "\n",
    "**n_heads** indicates the count of attention heads in the multi-head attention mechanism (see chapter 3).\n",
    "\n",
    "**n_layers** specifies the number of transformer blocks in the model, which we will cover in the upcoming discussion.\n",
    "\n",
    "**drop_rate** indicates the intensity of the dropout mechanism (0.1 implies a 10%\n",
    "ndom drop out of hidden units) to prevent overfitting (see chapter 3).\n",
    "\n",
    "**qkv_bias** determines whether to include a bias vector in the Linear layers of the multi-head attention for query, key, and value computations. We will initially disable this, following the norms of modern LLMs, but we will revisit it in chapter 6 when we load pretrained GPT-2 weights from OpenAI into our model (see chapter 6)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4498dad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        # Tokenize the entire text\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "        # Use a sliding window to chunk the book into overlapping sequences of max_length\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff0fb1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import dataloader\n",
    "\n",
    "class DummyGPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        \n",
    "        # Use a placeholder for TransformerBlock\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[DummyTransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "        \n",
    "        # Use a placeholder for LayerNorm\n",
    "        self.final_norm = DummyLayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
    "        )\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "class DummyTransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        # A simple placeholder\n",
    "\n",
    "    def forward(self, x):\n",
    "        # This block does nothing and just returns its input.\n",
    "        return x\n",
    "\n",
    "\n",
    "class DummyLayerNorm(nn.Module):\n",
    "    def __init__(self, normalized_shape, eps=1e-5):\n",
    "        super().__init__()\n",
    "        # The parameters here are just to mimic the LayerNorm interface.\n",
    "\n",
    "    def forward(self, x):\n",
    "        # This layer does nothing and just returns its input.\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23dc83b6",
   "metadata": {},
   "source": [
    "### STEP 1: TOKENIZATION    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2cac62b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "batch = []\n",
    "txt1 = \"Every effort moves you\"\n",
    "txt2 = \"Every day holds a\"\n",
    "batch.append(torch.tensor(tokenizer.encode(txt1)))\n",
    "batch.append(torch.tensor(tokenizer.encode(txt2)))\n",
    "batch = torch.stack(batch,dim=0)\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b961e33",
   "metadata": {},
   "source": [
    "### STEP 2: CREATE AN INSTANCE OF DUMMYGPTMODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ee34cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([2, 4, 50257])\n",
      "tensor([[[-1.2034,  0.3201, -0.7130,  ..., -1.5548, -0.2390, -0.4667],\n",
      "         [-0.1192,  0.4539, -0.4432,  ...,  0.2392,  1.3469,  1.2430],\n",
      "         [ 0.5307,  1.6720, -0.4695,  ...,  1.1966,  0.0111,  0.5835],\n",
      "         [ 0.0139,  1.6754, -0.3388,  ...,  1.1586, -0.0435, -1.0400]],\n",
      "\n",
      "        [[-1.0908,  0.1798, -0.9484,  ..., -1.6047,  0.2439, -0.4530],\n",
      "         [-0.7860,  0.5581, -0.0610,  ...,  0.4835, -0.0077,  1.6621],\n",
      "         [ 0.3567,  1.2698, -0.6398,  ..., -0.0162, -0.1296,  0.3717],\n",
      "         [-0.2407, -0.7349, -0.5102,  ...,  2.0057, -0.3694,  0.1814]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = DummyGPTModel(GPT_CONFIG_124M)\n",
    "logits = model(batch)\n",
    "print(\"Output shape:\", logits.shape)\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b81709",
   "metadata": {},
   "source": [
    "## GPT ARCHITECTURE PART 2: LAYER NORMALIZATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5518a8",
   "metadata": {},
   "source": [
    "#### Explanation with a simple example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08414564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2260, 0.3470, 0.0000, 0.2216, 0.0000, 0.0000],\n",
      "        [0.2133, 0.2394, 0.0000, 0.5198, 0.3297, 0.0000]],\n",
      "       grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "batch_example = torch.randn(2, 5) #A\n",
    "layer = nn.Sequential(nn.Linear(5, 6), nn.ReLU())\n",
    "out = layer(batch_example)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "91dbc358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:\n",
      " tensor([[0.1324],\n",
      "        [0.2170]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[0.0231],\n",
      "        [0.0398]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mean = out.mean(dim=-1, keepdim=True)\n",
    "var = out.var(dim=-1, keepdim=True)\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c44de788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:\n",
      " tensor([[9.9341e-09],\n",
      "        [0.0000e+00]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "out_norm= (out - mean)/torch.sqrt(var)\n",
    "mean = out_norm.mean(dim= -1, keepdim=True)\n",
    "var = out_norm.var(dim= -1, keepdim=True)\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fd148cd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:\n",
      " tensor([[0.0000],\n",
      "        [0.0000]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.set_printoptions(sci_mode=False)\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0996adba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self,emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps= 1e-5\n",
    "        self.sacle= nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift= nn.Parameter(torch.zeros(emb_dim))\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1,keepdim=True)\n",
    "        vars = x.var(dim=-1,keepdim=True , correction=False)\n",
    "        norm_x = (x- mean)/torch.sqrt(vars+self.eps)\n",
    "        return self.sacle*norm_x + self.shift\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fd246028",
   "metadata": {},
   "outputs": [],
   "source": [
    "ln = LayerNorm(emb_dim=5)\n",
    "out_ln = ln(batch_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4ed71d16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:\n",
      " tensor([[-0.0000],\n",
      "        [ 0.0000]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mean = out_ln.mean(dim=-1, keepdim=True)\n",
    "var = out_ln.var(dim=-1, unbiased=False, keepdim=True)\n",
    "\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a34a975",
   "metadata": {},
   "source": [
    "## GPT ARCHITECTURE PART 3: FEEDFORWARD NEURAL NETWORK WITH GELU ACTIVATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f1ef93",
   "metadata": {},
   "source": [
    "- GELU ([Hendrycks and Gimpel 2016](https://arxiv.org/abs/1606.08415)) can be implemented in several ways; the exact version is defined as GELU(x)=x⋅Φ(x), where Φ(x) is the cumulative distribution function of the standard Gaussian distribution.\n",
    "- In practice, it's common to implement a computationally cheaper approximation: $\\text{GELU}(x) \\approx 0.5 \\cdot x \\cdot \\left(1 + \\tanh\\left[\\sqrt{\\frac{2}{\\pi}} \\cdot \\left(x + 0.044715 \\cdot x^3\\right)\\right]\\right)\n",
    "$ (the original GPT-2 model was also trained with this approximation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "50137515",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0 / torch.pi)) * \n",
    "            (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "da6df281",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
    "            GELU(),\n",
    "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8caa93c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768\n"
     ]
    }
   ],
   "source": [
    "print(GPT_CONFIG_124M[\"emb_dim\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "013ac39c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 768])\n"
     ]
    }
   ],
   "source": [
    "ffn = FeedForward(GPT_CONFIG_124M)\n",
    "\n",
    "# input shape: [batch_size, num_token, emb_size]\n",
    "x = torch.rand(2, 3, 768) \n",
    "out = ffn(x)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7064473a",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch04_compressed/10.webp\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd425dca",
   "metadata": {},
   "source": [
    "## GPT ARCHITECTURE PART 4: SHORTCUT CONNECTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "265a96a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExampleDeepNeuralNetwork(nn.Module):\n",
    "    def __init__(self, layer_sizes, use_shortcut):\n",
    "        super().__init__()\n",
    "        self.use_shortcut = use_shortcut\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Sequential(nn.Linear(layer_sizes[0], layer_sizes[1]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[1], layer_sizes[2]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[2], layer_sizes[3]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[3], layer_sizes[4]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[4], layer_sizes[5]), GELU())\n",
    "        ])  \n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            # Compute the output of the current layer\n",
    "            layer_output = layer(x)\n",
    "            # Check if shortcut can be applied\n",
    "            if self.use_shortcut and x.shape == layer_output.shape:\n",
    "                x = x + layer_output\n",
    "            else:\n",
    "                x = layer_output\n",
    "        return x\n",
    "\n",
    "\n",
    "def print_gradients(model, x):\n",
    "    # Forward pass\n",
    "    output = model(x)\n",
    "    target = torch.tensor([[0.]])\n",
    "\n",
    "    # Calculate loss based on how close the target\n",
    "    # and output are\n",
    "    loss = nn.MSELoss()\n",
    "    loss = loss(output, target)\n",
    "    \n",
    "    # Backward pass to calculate the gradients\n",
    "    loss.backward()\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            # Print the mean absolute gradient of the weights\n",
    "            print(f\"{name} has gradient mean of {param.grad.abs().mean().item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e8311b3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.0.weight has gradient mean of 0.00020173587836325169\n",
      "layers.1.0.weight has gradient mean of 0.0001201116101583466\n",
      "layers.2.0.weight has gradient mean of 0.0007152041653171182\n",
      "layers.3.0.weight has gradient mean of 0.001398873864673078\n",
      "layers.4.0.weight has gradient mean of 0.005049646366387606\n"
     ]
    }
   ],
   "source": [
    "layer_sizes = [3, 3, 3, 3, 3, 1]  \n",
    "\n",
    "sample_input = torch.tensor([[1., 0., -1.]])\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model_without_shortcut = ExampleDeepNeuralNetwork(\n",
    "    layer_sizes, use_shortcut=False\n",
    ")\n",
    "print_gradients(model_without_shortcut, sample_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ff6adee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.0.weight has gradient mean of 0.22169792652130127\n",
      "layers.1.0.weight has gradient mean of 0.20694106817245483\n",
      "layers.2.0.weight has gradient mean of 0.32896995544433594\n",
      "layers.3.0.weight has gradient mean of 0.2665732502937317\n",
      "layers.4.0.weight has gradient mean of 1.3258541822433472\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model_with_shortcut = ExampleDeepNeuralNetwork(\n",
    "    layer_sizes, use_shortcut=True\n",
    ")\n",
    "print_gradients(model_with_shortcut, sample_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c97d94",
   "metadata": {},
   "source": [
    "## GPT ARCHITECTURE PART 5: CODING ATTENTION AND LINEAR LAYERS IN A TRANSFORMER BLOCK"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "82d46a54",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "147bf2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,    # Vocabulary size\n",
    "    \"context_length\": 1024, # Context length\n",
    "    \"emb_dim\": 768,         # Embedding dimension\n",
    "    \"n_heads\": 12,          # Number of attention heads\n",
    "    \"n_layers\": 12,         # Number of layers\n",
    "    \"drop_rate\": 0.1,       # Dropout rate\n",
    "    \"qkv_bias\": False       # Query-Key-Value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "415cd42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert (d_out % num_heads == 0), \\\n",
    "            \"d_out must be divisible by num_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads # Reduce the projection dim to match desired output dim\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\n",
    "            \"mask\",\n",
    "            torch.triu(torch.ones(context_length, context_length),\n",
    "                       diagonal=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "\n",
    "        keys = self.W_key(x) # Shape: (b, num_tokens, d_out)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        # We implicitly split the matrix by adding a `num_heads` dimension\n",
    "        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim) \n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
    "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
    "\n",
    "        # Original mask truncated to the number of tokens and converted to boolean\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "\n",
    "        # Use the mask to fill attention scores\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "        \n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2) \n",
    "        \n",
    "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec) # optional projection\n",
    "\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1fa55d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in=cfg[\"emb_dim\"],\n",
    "            d_out=cfg[\"emb_dim\"],\n",
    "            context_length=cfg[\"context_length\"],\n",
    "            num_heads=cfg[\"n_heads\"], \n",
    "            dropout=cfg[\"drop_rate\"],\n",
    "            qkv_bias=cfg[\"qkv_bias\"])\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Shortcut connection for attention block\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x)  # Shape [batch_size, num_tokens, emb_size]\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut  # Add the original input back\n",
    "\n",
    "        # Shortcut connection for feed forward block\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut  # Add the original input back\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d99560af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4, 768])\n",
      "Output shape: torch.Size([2, 4, 768])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "x = torch.rand(2, 4, 768) #A\n",
    "block = TransformerBlock(GPT_CONFIG_124M)\n",
    "output = block(x)\n",
    "print(\"Input shape:\", x.shape)\n",
    "print(\"Output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682a021f",
   "metadata": {},
   "source": [
    "## GPT ARCHITECTURE PART 6: ENTIRE GPT MODEL ARCHITECTURE IMPLEMENTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1f90caeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        \n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "        \n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
    "        )\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5e06a735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input batch:\n",
      " tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n",
      "\n",
      "Output shape: torch.Size([2, 4, 50257])\n",
      "tensor([[[ 0.3613,  0.4222, -0.0711,  ...,  0.3483,  0.4661, -0.2838],\n",
      "         [-0.1792, -0.5660, -0.9485,  ...,  0.0477,  0.5181, -0.3168],\n",
      "         [ 0.7120,  0.0332,  0.1085,  ...,  0.1018, -0.4327, -0.2553],\n",
      "         [-1.0076,  0.3418, -0.1190,  ...,  0.7195,  0.4023,  0.0532]],\n",
      "\n",
      "        [[-0.2564,  0.0900,  0.0335,  ...,  0.2659,  0.4454, -0.6806],\n",
      "         [ 0.1230,  0.3653, -0.2074,  ...,  0.7705,  0.2710,  0.2246],\n",
      "         [ 1.0558,  1.0318, -0.2800,  ...,  0.6936,  0.3205, -0.3178],\n",
      "         [-0.1565,  0.3926,  0.3288,  ...,  1.2630, -0.1858,  0.0388]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "out = model(batch)\n",
    "print(\"Input batch:\\n\", batch)\n",
    "print(\"\\nOutput shape:\", out.shape)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "87eb8c02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 163,009,536\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2517c610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token embedding layer shape: torch.Size([50257, 768])\n",
      "Output layer shape: torch.Size([50257, 768])\n"
     ]
    }
   ],
   "source": [
    "print(\"Token embedding layer shape:\", model.tok_emb.weight.shape)\n",
    "print(\"Output layer shape:\", model.out_head.weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "040c509e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters considering weight tying: 124,412,160\n"
     ]
    }
   ],
   "source": [
    "total_params_gpt2 = total_params - sum(p.numel() for p in model.out_head.parameters())\n",
    "print(f\"Number of trainable parameters considering weight tying: {total_params_gpt2:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bb7ffc7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size of the model: 621.83 MB\n"
     ]
    }
   ],
   "source": [
    "total_size_bytes = total_params * 4 #A\n",
    "total_size_mb = total_size_bytes / (1024 * 1024) #B\n",
    "print(f\"Total size of the model: {total_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f105d065",
   "metadata": {},
   "source": [
    "## GPT ARCHITECTURE PART 7: GENERATING TEXT FROM OUTPUT TOKENS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3f2ad37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
    "    # idx is (batch, n_tokens) array of indices in the current context\n",
    "    for _ in range(max_new_tokens):\n",
    "        \n",
    "        # Crop current context if it exceeds the supported context size\n",
    "        # E.g., if LLM supports only 5 tokens, and the context size is 10\n",
    "        # then only the last 5 tokens are used as context\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        \n",
    "        # Get the predictions\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        \n",
    "        # Focus only on the last time step\n",
    "        # (batch, n_tokens, vocab_size) becomes (batch, vocab_size)\n",
    "        logits = logits[:, -1, :]  \n",
    "\n",
    "        # Apply softmax to get probabilities\n",
    "        probas = torch.softmax(logits, dim=-1)  # (batch, vocab_size)\n",
    "\n",
    "        # Get the idx of the vocab entry with the highest probability value\n",
    "        idx_next = torch.argmax(probas, dim=-1, keepdim=True)  # (batch, 1)\n",
    "\n",
    "        # Append sampled index to the running sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)\n",
    "\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fd6c7d13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded: [15496, 11, 314, 716]\n",
      "encoded_tensor.shape: torch.Size([1, 4])\n"
     ]
    }
   ],
   "source": [
    "start_context = \"Hello, I am\"\n",
    "encoded = tokenizer.encode(start_context)\n",
    "print(\"encoded:\", encoded)\n",
    "encoded_tensor = torch.tensor(encoded).unsqueeze(0) #A\n",
    "print(\"encoded_tensor.shape:\", encoded_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "95e6d72e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: tensor([[15496,    11,   314,   716, 27018, 24086, 47843, 30961, 42348,  7267]])\n",
      "Output length: 10\n"
     ]
    }
   ],
   "source": [
    "model.eval() #A\n",
    "out = generate_text_simple(\n",
    "model=model,\n",
    "idx=encoded_tensor,\n",
    "max_new_tokens=6,\n",
    "context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "print(\"Output:\", out)\n",
    "print(\"Output length:\", len(out[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "62816ee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you Aeiman Byeswickattributeometer inspector Normandy freezerigrate\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0) # remove batch dimension\n",
    "    return tokenizer.decode(flat.tolist())\n",
    "\n",
    "start_context = \"Every effort moves you\"\n",
    "\n",
    "\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(start_context, tokenizer),\n",
    "    max_new_tokens=10,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cdc460b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, I am Featureiman Byeswickattribute argue\n"
     ]
    }
   ],
   "source": [
    "decoded_text = tokenizer.decode(out.squeeze(0).tolist())\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36315b7c",
   "metadata": {},
   "source": [
    "## GPT Architecture With all the blocks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "64b0d2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,    # Vocabulary size\n",
    "    \"context_length\": 1024, # Context length\n",
    "    \"emb_dim\": 768,         # Embedding dimension\n",
    "    \"n_heads\": 12,          # Number of attention heads\n",
    "    \"n_layers\": 12,         # Number of layers\n",
    "    \"drop_rate\": 0.1,       # Dropout rate\n",
    "    \"qkv_bias\": False       # Query-Key-Value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9a8eac74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class DummyGPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        \n",
    "        # Use a placeholder for TransformerBlock\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[DummyTransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "        \n",
    "        # Use a placeholder for LayerNorm\n",
    "        self.final_norm = DummyLayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
    "        )\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2dd29f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert (d_out % num_heads == 0), \\\n",
    "            \"d_out must be divisible by num_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads # Reduce the projection dim to match desired output dim\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\n",
    "            \"mask\",\n",
    "            torch.triu(torch.ones(context_length, context_length),\n",
    "                       diagonal=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "\n",
    "        keys = self.W_key(x) # Shape: (b, num_tokens, d_out)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim) \n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
    "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
    "\n",
    "        # Original mask truncated to the number of tokens and converted to boolean\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "\n",
    "        # Use the mask to fill attention scores\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "        \n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2) \n",
    "        \n",
    "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec) # optional projection\n",
    "\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "675f05e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift\n",
    "\n",
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0 / torch.pi)) * \n",
    "            (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]), ## Expansion\n",
    "            GELU(), ## Activation\n",
    "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]), ## Contraction\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "be21b206",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in=cfg[\"emb_dim\"],\n",
    "            d_out=cfg[\"emb_dim\"],\n",
    "            context_length=cfg[\"context_length\"],\n",
    "            num_heads=cfg[\"n_heads\"], \n",
    "            dropout=cfg[\"drop_rate\"],\n",
    "            qkv_bias=cfg[\"qkv_bias\"])\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Shortcut connection for attention block\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x)  # Shape [batch_size, num_tokens, emb_size]\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut  # Add the original input back\n",
    "\n",
    "        # Shortcut connection for feed forward block\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        # 2*4*768\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut  # Add the original input back\n",
    "\n",
    "        return x\n",
    "        # 2*4*768"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff74e1b",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch04_compressed/14.webp?1\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59acec3",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "Now, let us use all this knowledge and code the entire GPT architecture\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "92ee9ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        \n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "        \n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
    "        )\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bf84aa4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input batch:\n",
      " tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n",
      "\n",
      "Output shape: torch.Size([2, 4, 50257])\n",
      "tensor([[[ 0.3613,  0.4222, -0.0711,  ...,  0.3483,  0.4661, -0.2838],\n",
      "         [-0.1792, -0.5660, -0.9485,  ...,  0.0477,  0.5181, -0.3168],\n",
      "         [ 0.7120,  0.0332,  0.1085,  ...,  0.1018, -0.4327, -0.2553],\n",
      "         [-1.0076,  0.3418, -0.1190,  ...,  0.7195,  0.4023,  0.0532]],\n",
      "\n",
      "        [[-0.2564,  0.0900,  0.0335,  ...,  0.2659,  0.4454, -0.6806],\n",
      "         [ 0.1230,  0.3653, -0.2074,  ...,  0.7705,  0.2710,  0.2246],\n",
      "         [ 1.0558,  1.0318, -0.2800,  ...,  0.6936,  0.3205, -0.3178],\n",
      "         [-0.1565,  0.3926,  0.3288,  ...,  1.2630, -0.1858,  0.0388]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "out = model(batch)\n",
    "print(\"Input batch:\\n\", batch)\n",
    "print(\"\\nOutput shape:\", out.shape)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b2d3c6",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch04_compressed/15.webp\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1784a20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
    "    # idx is (batch, n_tokens) array of indices in the current context\n",
    "    for _ in range(max_new_tokens):\n",
    "        \n",
    "        # Crop current context if it exceeds the supported context size\n",
    "        # E.g., if LLM supports only 5 tokens, and the context size is 10\n",
    "        # then only the last 5 tokens are used as context\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        \n",
    "        # Get the predictions\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        \n",
    "        # Focus only on the last time step\n",
    "        # (batch, n_tokens, vocab_size) becomes (batch, vocab_size)\n",
    "        logits = logits[:, -1, :]  \n",
    "\n",
    "        # Apply softmax to get probabilities\n",
    "        probas = torch.softmax(logits, dim=-1)  # (batch, vocab_size)\n",
    "\n",
    "        # Get the idx of the vocab entry with the highest probability value\n",
    "        idx_next = torch.argmax(probas, dim=-1, keepdim=True)  # (batch, 1)\n",
    "\n",
    "        # Append sampled index to the running sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)\n",
    "\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "078b08ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor([[16833, 3626, 6100],   # [\"every effort moves\",\n",
    "                       [40,    1107, 588]])   #  \"I really like\"]\n",
    "\n",
    "targets = torch.tensor([[3626, 6100, 345  ],  # [\" effort moves you\",\n",
    "                        [1107,  588, 11311]]) #  \" really like chocolate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "90cb3ccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 50257])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    logits = model(inputs)\n",
    "\n",
    "probas = torch.softmax(logits, dim=-1) # Probability of each token in vocabulary\n",
    "print(probas.shape) # Shape: (batch_size, num_tokens, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0fa068b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "\n",
    "file_path = \"the-verdict.txt\"\n",
    "url = \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt\"\n",
    "\n",
    "if not os.path.exists(file_path):\n",
    "    with urllib.request.urlopen(url) as response:\n",
    "        text_data = response.read().decode('utf-8')\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(text_data)\n",
    "else:\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        text_data = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "273b7301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
     ]
    }
   ],
   "source": [
    "# First 100 characters\n",
    "print(text_data[:99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9b81d8f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it for me! The Strouds stand alone, and happen once--but there's no exterminating our kind of art.\"\n"
     ]
    }
   ],
   "source": [
    "# Last 100 characters\n",
    "print(text_data[-99:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c31cbce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters: 20479\n",
      "Tokens: 5145\n"
     ]
    }
   ],
   "source": [
    "total_characters = len(text_data)\n",
    "total_tokens = len(tokenizer.encode(text_data))\n",
    "\n",
    "print(\"Characters:\", total_characters)\n",
    "print(\"Tokens:\", total_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "18697971",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,    # Vocabulary size\n",
    "    \"context_length\": 256, # Context length\n",
    "    \"emb_dim\": 768,         # Embedding dimension\n",
    "    \"n_heads\": 12,          # Number of attention heads\n",
    "    \"n_layers\": 12,         # Number of layers\n",
    "    \"drop_rate\": 0.1,       # Dropout rate\n",
    "    \"qkv_bias\": False       # Query-Key-Value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a9a236a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader_v1(txt, batch_size=4, max_length=256, \n",
    "                         stride=128, shuffle=True, drop_last=True,\n",
    "                         num_workers=0):\n",
    "\n",
    "    # Initialize the tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    # Create dataset\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "\n",
    "    # Create dataloader\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5614ba91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/validation ratio\n",
    "train_ratio = 0.90\n",
    "split_idx = int(train_ratio * len(text_data))\n",
    "train_data = text_data[:split_idx]\n",
    "val_data = text_data[split_idx:]\n",
    "\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = create_dataloader_v1(\n",
    "    train_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = create_dataloader_v1(\n",
    "    val_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0acbc531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "\n",
      "Validation loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n"
     ]
    }
   ],
   "source": [
    "print(\"Train loader:\")\n",
    "for x, y in train_loader:\n",
    "    print(x.shape, y.shape)\n",
    "\n",
    "print(\"\\nValidation loader:\")\n",
    "for x, y in val_loader:\n",
    "    print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5957660f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "24964e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "    logits = model(input_batch)\n",
    "    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
    "    return loss\n",
    "\n",
    "\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        # Reduce the number of batches to match the total number of batches in the data loader\n",
    "        # if num_batches exceeds the number of batches in the data loader\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c9b1ff04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you Aeiman ByeswicknecUnSimply View Hispan garnered\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0) # remove batch dimension\n",
    "    return tokenizer.decode(flat.tolist())\n",
    "\n",
    "start_context = \"Every effort moves you\"\n",
    "\n",
    "\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(start_context, tokenizer),\n",
    "    max_new_tokens=10,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a13e5e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 10.986133363511827\n",
      "Validation loss: 11.01709270477295\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Note:\n",
    "# Uncommenting the following lines will allow the code to run on Apple Silicon chips, if applicable,\n",
    "# which is approximately 2x faster than on an Apple CPU (as measured on an M3 MacBook Air).\n",
    "# However, the resulting loss values may be slightly different.\n",
    "\n",
    "#if torch.cuda.is_available():\n",
    "#    device = torch.device(\"cuda\")\n",
    "#elif torch.backends.mps.is_available():\n",
    "#    device = torch.device(\"mps\")\n",
    "#else:\n",
    "#    device = torch.device(\"cpu\")\n",
    "#\n",
    "# print(f\"Using {device} device.\")\n",
    "\n",
    "\n",
    "model.to(device) # no assignment model = model.to(device) necessary for nn.Module classes\n",
    "\n",
    "\n",
    "torch.manual_seed(123) # For reproducibility due to the shuffling in the data loader\n",
    "\n",
    "with torch.no_grad(): # Disable gradient tracking for efficiency because we are not training, yet\n",
    "    train_loss = calc_loss_loader(train_loader, model, device)\n",
    "    val_loss = calc_loss_loader(val_loader, model, device)\n",
    "\n",
    "print(\"Training loss:\", train_loss)\n",
    "print(\"Validation loss:\", val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff02032",
   "metadata": {},
   "source": [
    "### Making the Trainling loop \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5b6f51b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "    model.train()\n",
    "    return train_loss, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ecfbc54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate_text_simple(\n",
    "            model=model, idx=encoded,\n",
    "            max_new_tokens=50, context_size=context_size\n",
    "        )\n",
    "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    print(decoded_text.replace(\"\\n\", \" \"))  # Compact print format\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f0f42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
    "                       eval_freq, eval_iter, start_context, tokenizer):\n",
    "    # Initialize lists to track losses and tokens seen\n",
    "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "    tokens_seen, global_step = 0, -1\n",
    "\n",
    "    # Main training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "        \n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad() # Reset loss gradients from previous batch iteration\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            loss.backward() # Calculate loss gradients\n",
    "            optimizer.step() # Update model weights using loss gradients\n",
    "            tokens_seen += input_batch.numel() # Returns the total number of elements (or tokens) in the input_batch.\n",
    "            global_step += 1\n",
    "\n",
    "            # Optional evaluation step\n",
    "            if global_step % eval_freq == 0: \n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, device, eval_iter)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
    "\n",
    "        # Print a sample text after each epoch\n",
    "        generate_and_print_sample(\n",
    "            model, tokenizer, device, start_context\n",
    "        )\n",
    "\n",
    "    return train_losses, val_losses, track_tokens_seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84040dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 9.781, Val loss 9.933\n",
      "Ep 1 (Step 000005): Train loss 8.111, Val loss 8.339\n",
      "Every effort moves you,,,,,,,,,,,,.                                     \n",
      "Ep 2 (Step 000010): Train loss 6.661, Val loss 7.048\n",
      "Ep 2 (Step 000015): Train loss 5.961, Val loss 6.616\n",
      "Every effort moves you, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and,, and, and,\n",
      "Ep 3 (Step 000020): Train loss 5.726, Val loss 6.600\n",
      "Ep 3 (Step 000025): Train loss 5.201, Val loss 6.348\n",
      "Every effort moves you, and I had been.                                            \n",
      "Ep 4 (Step 000030): Train loss 4.417, Val loss 6.278\n",
      "Ep 4 (Step 000035): Train loss 4.069, Val loss 6.226\n",
      "Every effort moves you know the                          \"I he had the donkey and I had the and I had the donkey and down the room, I had\n",
      "Ep 5 (Step 000040): Train loss 3.732, Val loss 6.160\n",
      "Every effort moves you know it was not that the picture--I had the fact by the last I had been--his, and in the            \"Oh, and he said, and down the room, and in\n",
      "Ep 6 (Step 000045): Train loss 2.850, Val loss 6.179\n",
      "Ep 6 (Step 000050): Train loss 2.427, Val loss 6.141\n",
      "Every effort moves you know,\" was one of the picture. The--I had a little of a little: \"Yes, and in fact, and in the picture was, and I had been at my elbow and as his pictures, and down the room, I had\n",
      "Ep 7 (Step 000055): Train loss 2.104, Val loss 6.134\n",
      "Ep 7 (Step 000060): Train loss 1.882, Val loss 6.233\n",
      "Every effort moves you know,\" was one of the picture for nothing--I told Mrs.  \"I was no--as! The women had been, in the moment--as Jack himself, as once one had been the donkey, and were, and in his\n",
      "Ep 8 (Step 000065): Train loss 1.320, Val loss 6.238\n",
      "Ep 8 (Step 000070): Train loss 0.985, Val loss 6.242\n",
      "Every effort moves you know,\" was one of the axioms he had been the tips of a self-confident moustache, I felt to see a smile behind his close grayish beard--as if he had the donkey. \"strongest,\" as his\n",
      "Ep 9 (Step 000075): Train loss 0.717, Val loss 6.293\n",
      "Ep 9 (Step 000080): Train loss 0.541, Val loss 6.393\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back the window-curtains, I had the donkey. \"There were days when I\n",
      "Ep 10 (Step 000085): Train loss 0.391, Val loss 6.452\n",
      "Every effort moves you know,\" was one of the axioms he laid down across the Sevres and silver of an exquisitely appointed luncheon-table, when, on a later day, I had again run over from Monte Carlo; and Mrs. Gis\n",
      "Training completed in 12.59 minutes.\n"
     ]
    }
   ],
   "source": [
    "# Note:\n",
    "# Uncomment the following code to calculate the execution time\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
    "\n",
    "num_epochs = 10\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model, train_loader, val_loader, optimizer, device,\n",
    "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
    "    start_context=\"Every effort moves you\", tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# Note:\n",
    "# Uncomment the following code to show the execution time\n",
    "end_time = time.time()\n",
    "execution_time_minutes = (end_time - start_time) / 60\n",
    "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4fcf5054",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'num_epochs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[63]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     21\u001b[39m     fig.tight_layout()  \u001b[38;5;66;03m# Adjust layout to make room\u001b[39;00m\n\u001b[32m     22\u001b[39m     plt.show()\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m epochs_tensor = torch.linspace(\u001b[32m0\u001b[39m, \u001b[43mnum_epochs\u001b[49m, \u001b[38;5;28mlen\u001b[39m(train_losses))\n\u001b[32m     25\u001b[39m plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)\n",
      "\u001b[31mNameError\u001b[39m: name 'num_epochs' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "\n",
    "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
    "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
    "\n",
    "    # Plot training and validation loss against epochs\n",
    "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
    "    ax1.plot(epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\")\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.legend(loc=\"upper right\")\n",
    "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))  # only show integer labels on x-axis\n",
    "\n",
    "    # Create a second x-axis for tokens seen\n",
    "    ax2 = ax1.twiny()  # Create a second x-axis that shares the same y-axis\n",
    "    ax2.plot(tokens_seen, train_losses, alpha=0)  # Invisible plot for aligning ticks\n",
    "    ax2.set_xlabel(\"Tokens seen\")\n",
    "\n",
    "    fig.tight_layout()  # Adjust layout to make room\n",
    "    plt.show()\n",
    "\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d702ac0",
   "metadata": {},
   "source": [
    "## DECODING STRATEGIES TO CONTROL RANDOMNESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3eb78f8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(1024, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(\"cpu\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f8b0bd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "vocab = { \n",
    "    \"closer\": 0,\n",
    "    \"every\": 1, \n",
    "    \"effort\": 2, \n",
    "    \"forward\": 3,\n",
    "    \"inches\": 4,\n",
    "    \"moves\": 5, \n",
    "    \"pizza\": 6,\n",
    "    \"toward\": 7,\n",
    "    \"you\": 8,\n",
    "} \n",
    "\n",
    "inverse_vocab = {v: k for k, v in vocab.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "aaae4be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_token_logits = torch.tensor(\n",
    "[4.51, 0.89, -1.90, 6.75, 1.63, -1.62, -1.89, 6.28, 1.79]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "39dc62e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward\n"
     ]
    }
   ],
   "source": [
    "probas = torch.softmax(next_token_logits, dim=0)\n",
    "next_token_id = torch.argmax(probas).item()\n",
    "print(inverse_vocab[next_token_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "73b42dbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 x closer\n",
      "0 x every\n",
      "0 x effort\n",
      "1000 x forward\n"
     ]
    }
   ],
   "source": [
    "def print_sampled_tokens(probas):\n",
    "    torch.manual_seed(123) # Manual seed for reproducibility\n",
    "    sample = [torch.argmax(probas, dim=-1).item() for i in range(1_000)]\n",
    "    sampled_ids = torch.bincount(torch.tensor(sample))\n",
    "    for i, freq in enumerate(sampled_ids):\n",
    "        print(f\"{freq} x {inverse_vocab[i]}\")\n",
    "\n",
    "print_sampled_tokens(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "226d76da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "next_token_id = torch.multinomial(probas, num_samples=1).item()\n",
    "print(inverse_vocab[next_token_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b6e90a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 x closer\n",
      "0 x every\n",
      "0 x effort\n",
      "1000 x forward\n"
     ]
    }
   ],
   "source": [
    "def print_sampled_tokens(probas):\n",
    "    torch.manual_seed(123) # Manual seed for reproducibility\n",
    "    sample = [torch.argmax(probas, dim=-1).item() for i in range(1_000)]\n",
    "    sampled_ids = torch.bincount(torch.tensor(sample))\n",
    "    for i, freq in enumerate(sampled_ids):\n",
    "        print(f\"{freq} x {inverse_vocab[i]}\")\n",
    "\n",
    "print_sampled_tokens(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "bbdf1332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73 x closer\n",
      "0 x every\n",
      "0 x effort\n",
      "582 x forward\n",
      "2 x inches\n",
      "0 x moves\n",
      "0 x pizza\n",
      "343 x toward\n"
     ]
    }
   ],
   "source": [
    "def print_sampled_tokens(probas):\n",
    "    torch.manual_seed(123) # Manual seed for reproducibility\n",
    "    sample = [torch.multinomial(probas, num_samples=1).item() for i in range(1_000)]\n",
    "    sampled_ids = torch.bincount(torch.tensor(sample))\n",
    "    for i, freq in enumerate(sampled_ids):\n",
    "        print(f\"{freq} x {inverse_vocab[i]}\")\n",
    "\n",
    "print_sampled_tokens(probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67c7f50",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "The printed output is \"forward\" just like before. What happened? The multinomial\n",
    "function samples the next token proportional to its probability score. \n",
    "\n",
    "In other words,\n",
    "\"forward\" is still the most likely token and will be selected by multinomial most of the\n",
    "time but not all the time. \n",
    "\n",
    "To illustrate this, let's implement a function that repeats this\n",
    "sampling 1000 times:\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9db686f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_with_temperature(logits, temperature):\n",
    "    scaled_logits = logits / temperature\n",
    "    return torch.softmax(scaled_logits, dim=0)\n",
    "\n",
    "# Temperature values\n",
    "temperatures = [1, 0.1, 5]  # Original, higher confidence, and lower confidence\n",
    "\n",
    "# Calculate scaled probabilities\n",
    "scaled_probas = [softmax_with_temperature(next_token_logits, T) for T in temperatures]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "094ccadf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting\n",
    "x = torch.arange(len(vocab))\n",
    "bar_width = 0.15\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 3))\n",
    "for i, T in enumerate(temperatures):\n",
    "    rects = ax.bar(x + i * bar_width, scaled_probas[i], bar_width, label=f'Temperature = {T}')\n",
    "\n",
    "ax.set_ylabel('Probability')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(vocab.keys(), rotation=90)\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026fb21d",
   "metadata": {},
   "source": [
    "### DECODING STRATEGY 2: Top-k sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6a9950c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_token_logits = torch.tensor(\n",
    "[4.51, 0.89, -1.90, 6.75, 1.63, -1.62, -1.89, 6.28, 1.79]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d36083ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top logits: tensor([6.7500, 6.2800, 4.5100])\n",
      "Top positions: tensor([3, 7, 0])\n"
     ]
    }
   ],
   "source": [
    "top_k = 3\n",
    "top_logits, top_pos = torch.topk(next_token_logits, top_k)\n",
    "print(\"Top logits:\", top_logits)\n",
    "print(\"Top positions:\", top_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "986df7be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4.5100,   -inf,   -inf, 6.7500,   -inf,   -inf,   -inf, 6.2800,   -inf])\n"
     ]
    }
   ],
   "source": [
    "new_logits = torch.where(\n",
    "    condition=next_token_logits < top_logits[-1],\n",
    "    input=torch.tensor(float(\"-inf\")), \n",
    "    other=next_token_logits\n",
    ")\n",
    "\n",
    "print(new_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4f7e4f3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0615, 0.0000, 0.0000, 0.5775, 0.0000, 0.0000, 0.0000, 0.3610, 0.0000])\n"
     ]
    }
   ],
   "source": [
    "topk_probas = torch.softmax(new_logits, dim=0)\n",
    "print(topk_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e4a2d9",
   "metadata": {},
   "source": [
    "### Merge Temperature Scaling and Top-k sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "33a6f79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n",
    "\n",
    "    # For-loop is the same as before: Get logits, and only focus on last time step\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        logits = logits[:, -1, :]\n",
    "\n",
    "        # New: Filter logits with top_k sampling\n",
    "        if top_k is not None:\n",
    "            # Keep only top_k values\n",
    "            top_logits, _ = torch.topk(logits, top_k)\n",
    "            min_val = top_logits[:, -1]\n",
    "            logits = torch.where(logits < min_val, torch.tensor(float(\"-inf\")).to(logits.device), logits)\n",
    "\n",
    "        # New: Apply temperature scaling\n",
    "        if temperature > 0.0:\n",
    "            logits = logits / temperature\n",
    "\n",
    "            # Apply softmax to get probabilities\n",
    "            probs = torch.softmax(logits, dim=-1)  # (batch_size, context_len)\n",
    "\n",
    "            # Sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)\n",
    "\n",
    "        # Otherwise same as before: get idx of the vocab entry with the highest logits value\n",
    "        else:\n",
    "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)\n",
    "\n",
    "        if idx_next == eos_id:  # Stop generating early if end-of-sequence token is encountered and eos_id is specified\n",
    "            break\n",
    "\n",
    "        # Same as before: append sampled index to the running sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens+1)\n",
    "\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "363fa064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you JCFord Run Rates gorePutting Settlement 330encingSee Cruiser41 international donatedabsolute\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "    max_new_tokens=15,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"],\n",
    "    top_k=25,\n",
    "    temperature=1.4\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db824abc",
   "metadata": {},
   "source": [
    "## LOADING AND SAVING MODEL WEIGHTS IN PYTORCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4b6a6786",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "torch.save(model.state_dict(), \"model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f96f2783",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.load_state_dict(torch.load(\"model.pth\"))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b4337bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
    "\n",
    "torch.save({\n",
    "    \"model_state_dict\": model.state_dict(),\n",
    "    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "    }, \n",
    "    \"model_and_optimizer.pth\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "2386b5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(\"model_and_optimizer.pth\")\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=0.1)\n",
    "optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "model.train();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "dff6704f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.20.0\n",
      "tqdm version: 4.67.3\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tqdm\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"tqdm version:\", tqdm.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1dd2d3",
   "metadata": {},
   "source": [
    "## LOADING PRETRAINED WEIGHTS FROM OPENAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6924f75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests  # Make sure requests is installed\n",
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24fdcfa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e4c79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_load_gpt2(model_size, models_dir):\n",
    "    allowed_sizes = (\"124M\", \"355M\", \"774M\", \"1558M\")\n",
    "    if model_size not in allowed_sizes:\n",
    "        raise ValueError(f\"Model size not in {allowed_sizes}\")\n",
    "\n",
    "    # Path setup: Weights/gpt2/124M\n",
    "    model_dir = os.path.join(models_dir, \"gpt2\", model_size)\n",
    "    base_url = \"https://openaipublic.blob.core.windows.net/gpt-2/models\"\n",
    "    filenames = [\n",
    "        \"checkpoint\", \"encoder.json\", \"hparams.json\",\n",
    "        \"model.ckpt.data-00000-of-00001\", \"model.ckpt.index\",\n",
    "        \"model.ckpt.meta\", \"vocab.bpe\"\n",
    "    ]\n",
    "\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    for filename in filenames:\n",
    "        file_url = os.path.join(base_url, model_size, filename)\n",
    "        file_path = os.path.join(model_dir, filename)\n",
    "        \n",
    "        # Check if file already exists before downloading\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"Downloading {filename}...\")\n",
    "            download_file(file_url, file_path)\n",
    "        else:\n",
    "            print(f\"{filename} already exists. Skipping.\")\n",
    "\n",
    "    # Load from the verified directory\n",
    "    tf_ckpt_path = tf.train.latest_checkpoint(model_dir)\n",
    "    settings = json.load(open(os.path.join(model_dir, \"hparams.json\")))\n",
    "    params = load_gpt2_params_from_tf_ckpt(tf_ckpt_path, settings)\n",
    "\n",
    "    return settings, params\n",
    "def download_file(url, destination):\n",
    "    try:\n",
    "        # Send a GET request to download the file, disabling SSL verification\n",
    "        response = requests.get(url, stream=True, verify=False)\n",
    "\n",
    "        # Get the total file size from headers, defaulting to 0 if not present\n",
    "        file_size = int(response.headers.get(\"content-length\", 0))\n",
    "\n",
    "        # Check if file exists and has the same size\n",
    "        if os.path.exists(destination):\n",
    "            file_size_local = os.path.getsize(destination)\n",
    "            if file_size == file_size_local:\n",
    "                print(f\"File already exists and is up-to-date: {destination}\")\n",
    "                return\n",
    "\n",
    "        # Define the block size for reading the file\n",
    "        block_size = 1024  # 1 Kilobyte\n",
    "\n",
    "        # Initialize the progress bar with total file size\n",
    "        progress_bar_description = url.split(\"/\")[-1]  # Extract filename from URL\n",
    "        with tqdm(total=file_size, unit=\"iB\", unit_scale=True, desc=progress_bar_description) as progress_bar:\n",
    "            # Open the destination file in binary write mode\n",
    "            with open(destination, \"wb\") as file:\n",
    "                # Iterate over the file data in chunks\n",
    "                for chunk in response.iter_content(block_size):\n",
    "                    progress_bar.update(len(chunk))  # Update progress bar\n",
    "                    file.write(chunk)  # Write the chunk to the file\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error downloading the file: {e}\")\n",
    "        print(f\"Please check the URL: {url}\")\n",
    "\n",
    "def load_gpt2_params_from_tf_ckpt(ckpt_path, settings):\n",
    "    # Initialize parameters dictionary with empty blocks for each layer\n",
    "    params = {\"blocks\": [{} for _ in range(settings[\"n_layer\"])]}\n",
    "\n",
    "    # Iterate over each variable in the checkpoint\n",
    "    for name, _ in tf.train.list_variables(ckpt_path):\n",
    "        # Load the variable and remove singleton dimensions\n",
    "        variable_array = np.squeeze(tf.train.load_variable(ckpt_path, name))\n",
    "\n",
    "        # Process the variable name to extract relevant parts\n",
    "        variable_name_parts = name.split(\"/\")[1:]  # Skip the 'model/' prefix\n",
    "\n",
    "        # Identify the target dictionary for the variable\n",
    "        target_dict = params\n",
    "        if variable_name_parts[0].startswith(\"h\"):\n",
    "            layer_number = int(variable_name_parts[0][1:])\n",
    "            target_dict = params[\"blocks\"][layer_number]\n",
    "\n",
    "        # Recursively access or create nested dictionaries\n",
    "        for key in variable_name_parts[1:-1]:\n",
    "            target_dict = target_dict.setdefault(key, {})\n",
    "\n",
    "        # Assign the variable array to the last key\n",
    "        last_key = variable_name_parts[-1]\n",
    "        target_dict[last_key] = variable_array\n",
    "\n",
    "    return params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d489b84e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint already exists. Skipping.\n",
      "encoder.json already exists. Skipping.\n",
      "hparams.json already exists. Skipping.\n",
      "model.ckpt.data-00000-of-00001 already exists. Skipping.\n",
      "model.ckpt.index already exists. Skipping.\n",
      "model.ckpt.meta already exists. Skipping.\n",
      "vocab.bpe already exists. Skipping.\n",
      "Model settings: {'n_vocab': 50257, 'n_ctx': 1024, 'n_embd': 768, 'n_head': 12, 'n_layer': 12}\n"
     ]
    }
   ],
   "source": [
    "settings, params = download_and_load_gpt2(model_size=\"124M\", models_dir=\"../Weights\")\n",
    "print(\"Model settings:\", settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47d6b30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Settings: {'n_vocab': 50257, 'n_ctx': 1024, 'n_embd': 768, 'n_head': 12, 'n_layer': 12}\n",
      "Parameter dictionary keys: dict_keys(['blocks', 'b', 'g', 'wpe', 'wte'])\n"
     ]
    }
   ],
   "source": [
    "print(\"Settings:\", settings)\n",
    "print(\"Parameter dictionary keys:\", params.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfe1ebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.11010301 -0.03926672  0.03310751 ... -0.1363697   0.01506208\n",
      "   0.04531523]\n",
      " [ 0.04034033 -0.04861503  0.04624869 ...  0.08605453  0.00253983\n",
      "   0.04318958]\n",
      " [-0.12746179  0.04793796  0.18410145 ...  0.08991534 -0.12972379\n",
      "  -0.08785918]\n",
      " ...\n",
      " [-0.04453601 -0.05483596  0.01225674 ...  0.10435229  0.09783269\n",
      "  -0.06952604]\n",
      " [ 0.1860082   0.01665728  0.04611587 ... -0.09625227  0.07847701\n",
      "  -0.02245961]\n",
      " [ 0.05135201 -0.02768905  0.0499369  ...  0.00704835  0.15519823\n",
      "   0.12067825]]\n",
      "Token embedding weight tensor dimensions: (50257, 768)\n"
     ]
    }
   ],
   "source": [
    "print(params[\"wte\"])\n",
    "print(\"Token embedding weight tensor dimensions:\", params[\"wte\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21321e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model configurations in a dictionary for compactness\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "\n",
    "# Copy the base configuration and update with specific model settings\n",
    "model_name = \"gpt2-small (124M)\"  # Example model name\n",
    "NEW_CONFIG = GPT_CONFIG_124M.copy()\n",
    "NEW_CONFIG.update(model_configs[model_name])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69524a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "NEW_CONFIG.update({\"context_length\": 1024, \"qkv_bias\": True})\n",
    "gpt = GPTModel(NEW_CONFIG)\n",
    "gpt.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0882b7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign(left, right):\n",
    "    if left.shape != right.shape:\n",
    "        raise ValueError(f\"Shape mismatch. Left: {left.shape}, Right: {right.shape}\")\n",
    "    return torch.nn.Parameter(torch.tensor(right))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c940594",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_weights_into_gpt(gpt, params):\n",
    "    gpt.pos_emb.weight = assign(gpt.pos_emb.weight, params['wpe'])\n",
    "    gpt.tok_emb.weight = assign(gpt.tok_emb.weight, params['wte'])\n",
    "    \n",
    "    for b in range(len(params[\"blocks\"])):\n",
    "        q_w, k_w, v_w = np.split(\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"w\"], 3, axis=-1)\n",
    "        gpt.trf_blocks[b].att.W_query.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_query.weight, q_w.T)\n",
    "        gpt.trf_blocks[b].att.W_key.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_key.weight, k_w.T)\n",
    "        gpt.trf_blocks[b].att.W_value.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_value.weight, v_w.T)\n",
    "\n",
    "        q_b, k_b, v_b = np.split(\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"b\"], 3, axis=-1)\n",
    "        gpt.trf_blocks[b].att.W_query.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_query.bias, q_b)\n",
    "        gpt.trf_blocks[b].att.W_key.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_key.bias, k_b)\n",
    "        gpt.trf_blocks[b].att.W_value.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_value.bias, v_b)\n",
    "\n",
    "        gpt.trf_blocks[b].att.out_proj.weight = assign(\n",
    "            gpt.trf_blocks[b].att.out_proj.weight, \n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].att.out_proj.bias = assign(\n",
    "            gpt.trf_blocks[b].att.out_proj.bias, \n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "        gpt.trf_blocks[b].ff.layers[0].weight = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[0].weight, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].ff.layers[0].bias = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[0].bias, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"])\n",
    "        gpt.trf_blocks[b].ff.layers[2].weight = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[2].weight, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].ff.layers[2].bias = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[2].bias, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "        gpt.trf_blocks[b].norm1.scale = assign(\n",
    "            gpt.trf_blocks[b].norm1.scale, \n",
    "            params[\"blocks\"][b][\"ln_1\"][\"g\"])\n",
    "        gpt.trf_blocks[b].norm1.shift = assign(\n",
    "            gpt.trf_blocks[b].norm1.shift, \n",
    "            params[\"blocks\"][b][\"ln_1\"][\"b\"])\n",
    "        gpt.trf_blocks[b].norm2.scale = assign(\n",
    "            gpt.trf_blocks[b].norm2.scale, \n",
    "            params[\"blocks\"][b][\"ln_2\"][\"g\"])\n",
    "        gpt.trf_blocks[b].norm2.shift = assign(\n",
    "            gpt.trf_blocks[b].norm2.shift, \n",
    "            params[\"blocks\"][b][\"ln_2\"][\"b\"])\n",
    "\n",
    "    gpt.final_norm.scale = assign(gpt.final_norm.scale, params[\"g\"])\n",
    "    gpt.final_norm.shift = assign(gpt.final_norm.shift, params[\"b\"])\n",
    "    gpt.out_head.weight = assign(gpt.out_head.weight, params[\"wte\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f440ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_weights_into_gpt(gpt, params)\n",
    "gpt.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f274561b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you toward finding an ideal new way down. By helping to establish your brand identity at every step, you'll know if these tactics\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "token_ids = generate(\n",
    "    model=gpt,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer).to(device),\n",
    "    max_new_tokens=25,\n",
    "    context_size=NEW_CONFIG[\"context_length\"],\n",
    "    top_k=50,\n",
    "    temperature=1.6\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt-from-scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
